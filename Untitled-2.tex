% 文档类与基础设置
\documentclass[12pt,a4paper,oneside]{article} % 单页排版，适合示例
\usepackage[margin=2.5cm]{geometry} % 页边距设置
\usepackage{ctex} % 中文支持
\usepackage{amsmath,amssymb,amsthm} % 数学公式与定理环境
\usepackage{booktabs,multirow,tabularx} % 表格工具
\usepackage{enumitem} % 列表自定义
\usepackage{hyperref} % 超链接
\usepackage{listings} % 代码块
\usepackage{xcolor} % 颜色支持（用于代码高亮）
\usepackage{lipsum} % 生成示例文本（可替换为实际内容）
\usepackage{graphicx} % 预留图片包（示例中不使用外部图）
\usepackage{float} % 固定表格/图片位置
\usepackage{kpfonts}
\usepackage{subcaption}
\usepackage{graphicx}
% 自定义配置
% 定理环境定义
\newtheorem{theorem}{定理}[section]
\newtheorem{definition}{定义}[section]
\newtheorem{lemma}{引理}[section]
\newtheorem{corollary}{推论}[section]
\newtheorem{example}{示例}[section]

% 超链接样式
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=blue,
    urlcolor=blue
}

% 代码块样式（Python示例）
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    showspaces=false
}




\begin{document}

% 标题页
\begin{titlepage}
    \centering
    \vspace*{3cm}
    
    {\Huge \bfseries   基于LaTeX的学术论文排版示例}\\[0.8cm]%下一行距离
    {\large \bfseries 包含公式、表格、代码与定理环境的完整模板}\\[2cm]
    
    {\Large \textbf{作者：  }\quad \quad  LaTeX学习者}\\[0.5cm]%添加空格
    {\Large \textbf{日期：} \today}\\[3cm]
    
    {\large 本示例展示学术论文的标准LaTeX实现方式\\
    所有内容均可直接编译，无需外部文件依赖}
    
    \vfill%填充空白到页面底部
    \end{titlepage}


% 目录
\tableofcontents
\newpage


% 正文部分
\section{引言}
\subsection{研究背景}
学术论文的规范化排版是科研成果展示的重要环节。LaTeX作为一种基于标记语言的排版系统，凭借其对数学公式、图表、参考文献的强大支持，成为理工科论文的首选工具。
本示例通过一个虚拟的"数值计算算法研究"主题，展示LaTeX在论文写作中的核心应用，包括：
\begin{enumerate}[label=(\alph*)]
   \item   复杂数学公式的编辑（含行内公式、独立公式、矩阵等）；
 \item 多样化表格的设计（三线表、跨行列表格、多列表格）；
  \item  定理、定义等学术元素的格式化呈现；
  \item  代码块的插入与高亮设置；
  \item \quad 参考文献的引用与管理。
\end{enumerate}

\subsection{研究内容}
{  本文以{ \bfseries"改进的梯度下降算法"}为研究对象，通过理论推导与数值实验验证算法性能。主要工作包括：

1. 推导梯度下降算法的收敛性条件；

2. 提出一种自适应学习率调整策略；

3. 通过数值实验对比改进算法与传统算法的收敛速度。
}


\section{理论基础}
\subsection{梯度下降算法定义}
\begin{definition}[梯度下降]
设目标函数$f:\mathbb{R}^n \to \mathbb{R}$为连续可微函数，梯度下降算法通过以下迭代公式寻找函数极小值点：

    $$\theta_{k+1} = \theta_k - \eta \nabla f(\theta_k)$$

其中：
- $\theta_k \in \mathbb{R}^n$为第$k$次迭代的参数向量；
- $\eta > 0$为学习率（步长）；
- $\nabla f(\theta_k) = \left( \frac{\partial f}{\partial \theta_1}, \cdots, \frac{\partial f}{\partial \theta_n} \right)^T$为$f$在$\theta_k$处的梯度。
\end{definition}

\subsection{收敛性定理}
\begin{theorem}[收敛性条件]
若目标函数$f$满足：
1.  Lipschitz连续梯度条件：存在$L > 0$，使得对任意$\theta, \theta' \in \mathbb{R}^n$，有
   \[
   \|\nabla f(\theta) - \nabla f(\theta')\| \leq L \|\theta - \theta'\|
   \]
2. 学习率满足$0 < \eta < \frac{2}{L}$，

则梯度下降算法生成的序列$\{\theta_k\}$收敛至$f$的极小值点，且收敛速度为线性。
\end{theorem}
\begin{theorem}[收敛性条件]
若目标函数$f$满足：
1.  Lipschitz连续梯度条件：存在$L > 0$，使得对任意$\theta, \theta' \in \mathbb{R}^n$，有
   \[
   \|\nabla f(\theta) - \nabla f(\theta')\| \leq L \|\theta - \theta'\|
   \]
2. 学习率满足$0 < \eta < \frac{2}{L}$，

则梯度下降算法生成的序列$\{\theta_k\}$收敛至$f$的极小值点，且收敛速度为线性。
\end{theorem}
\begin{lemma}[梯度有界性]
在定理2.1的条件下，梯度序列$\{\nabla f(\theta_k)\}$满足：
\[
\|\nabla f(\theta_k)\|^2 \leq \frac{2L}{k+1} \left( f(\theta_0) - f(\theta^*) \right)
\]
其中$\theta^*$为$f$的极小值点。
\end{lemma}

\begin{corollary}
当$k \to \infty$时，$\|\nabla f(\theta_k)\| \to 0$，即参数序列收敛至平稳点。
\end{corollary}


\section{改进算法设计}
\subsection{自适应学习率策略}
传统梯度下降算法的学习率$\eta$为固定值，难以兼顾收敛速度与稳定性。本文提出一种基于梯度模长的自适应调整策略：
\[
\eta_{k+1} = 
\begin{cases} 
\min(\eta_k \cdot 1.2, \eta_{\text{max}}) & \text{若} \|\nabla f(\theta_{k+1})\| < \|\nabla f(\theta_k)\|, \\
\max(\eta_k \cdot 0.5, \eta_{\text{min}}) & \text{否则},
\end{cases}
\]
其中$\eta_{\text{max}} = 0.1$，$\eta_{\text{min}} = 10^{-5}$为学习率上下限。

\subsection{算法伪代码}
改进算法的伪代码如下：
\begin{lstlisting}[caption={自适应梯度下降算法}, label={alg:agd}]
def adaptive_gradient_descent(f, grad_f, theta0, max_iter=1000):
    theta = theta0  # 初始参数
    eta = 0.01      # 初始学习率
    eta_max = 0.1   # 最大学习率
    eta_min = 1e-5  # 最小学习率
    grad_prev = grad_f(theta)  # 初始梯度
    
    for k in range(max_iter):
        # 参数更新
        theta_new = theta - eta * grad_prev
        
        # 计算新梯度
        grad_new = grad_f(theta_new)
        
        # 自适应调整学习率
        if np.linalg.norm(grad_new) < np.linalg.norm(grad_prev):
            eta = min(eta * 1.2, eta_max)
        else:
            eta = max(eta * 0.5, eta_min)
        
        # 收敛判断
        if np.linalg.norm(grad_new) < 1e-6:
            break
            
        theta, grad_prev = theta_new, grad_new
    
    return theta
\end{lstlisting}


\section{实验结果与分析}
\subsection{实验设置}
实验采用二次函数$f(\theta) = \frac{1}{2} \theta^T A \theta + b^T \theta$作为测试函数，其中：

{\[
A = \begin{pmatrix} 
2 & 1 \\ 
1 & 3 
\end{pmatrix}, \quad b = \begin{pmatrix} 
    -1 & 2 \\
    2  &3 
\end{pmatrix}
\]}\\[0.2cm]
\quad 该函数的理论极小值点为$\theta^* = A^{-1}(-b) = (0.8, -0.4)^T$。
\\对比算法包括：

- 传统梯度下降（固定学习率$\eta=0.05$）；

- 本文提出的自适应梯度下降（初始$\eta=0.05$）。

评价指标为迭代次数（达到$\|\nabla f(\theta)\| < 10^{-6}$时的步数）。

\subsection{实验结果对比}
不同初始点下的实验结果如表\ref{tab:exp_results}所示：

\begin{table}[H] % 固定表格位置
    \centering % 表格整体相对于页面居中
    \caption{两种算法的收敛迭代次数对比} % 去掉标题前多余空格，更规范
    \label{tab:exp_results}
    % 调整列格式：用"c c c c"替代"c c c X"，避免X列自适应导致的视觉偏移
    % 若需控制表格宽度，可添加\resizebox确保表格不超页宽，同时保持居中
    \resizebox{0.8\textwidth}{!}{% 可选：将表格缩放到页面宽度的80%，!表示保持宽高比
        \begin{tabular}{c c c c} % 4列均为居中对齐，无自适应列，整体更对称
            \toprule % 三线表顶部粗线
            初始点
$\theta_0$ & 传统梯度下降 & 自适应梯度下降 & 性能提升比例 \\
            \midrule % 三线表中间细线
            $(0, 0)^T$       & 128          & 42             & 67.2\%       \\
            $(5, 5)^T$       & 312          & 58             & 81.4\%       \\
            $(-3, 2)^T$      & 256          & 49             & 80.9\%       \\
            \midrule % 分隔“数据行”与“平均行”
            平均             & 232          & 49.7           & 78.6\%       \\
            \bottomrule % 三线表底部粗线
        \end{tabular}
    }
\end{table}

\begin{example}[单次实验细节]
当初始点为$\theta_0 = (5, 5)^T$时：
- 传统算法在第312步收敛，最终参数为$(0.8001, -0.3998)^T$；
- 自适应算法在第58步收敛，最终参数为$(0.7999, -0.4001)^T$，与理论值误差小于$10^{-4}$。
\end{example}

% 正文档文档环境中插入图片
\begin{figure}[H]  % [H] 固定图片位置（避免浮动）
    \centering  % 图片居中
    % 关键：Windows 路径中的反斜杠 \ 需替换为斜杠杠 /，或用双个个反杠 \\
    \includegraphics[width=0.1\textwidth]{C:/Users/Xing/Desktop/微信图片_20250919214155_17_153.jpg}
    \caption{本人图片}  % 图片标题
    \label{fig:wechat}  % 图片标签（用于交叉引用，如 \ref{fig:wechat}）
\end{figure}
% 两张图片并排居中显示
\begin{figure}[H]
    \centering  % 整体居中（含两侧空白）
    \hspace{0.10\textwidth}
    % 子图宽度改为 0.4\textwidth（总宽度 80%，两侧各留 10% 空白）
    \begin{subfigure}[b]{0.3\textwidth}  % 比之前的 0.45 略窄
        \centering
        \includegraphics[width=\textwidth]{C:/Users/Xing/Desktop/微信图片_20250919214155_17_153.jpg}
        \caption{第一张图片标题}
        \label{fig:sub1}
    \end{subfigure}
    \hfill  % 子图间间距
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{C:/Users/Xing/Desktop/微信图片_20250919214155_17_153.jpg}
        \caption{第二张图片标题}
        \label{fig:sub2}
    
    \end{subfigure}
    \hspace{0.10\textwidth}
    \caption{两张图片的整体标题}
    \label{fig:total}
\end{figure}
\subsection{结果分析}
从表\ref{tab:exp_results}可见：
1. 自适应梯度下降算法的收敛速度显著优于传统算法，平均迭代次数减少78.6%；
2. 当初始点远离极小值时（如$(5,5)^T$），自适应算法的优势更明显（性能提升81.4%）；
3. 两种算法最终都能收敛至理论极小值附近，说明自适应策略未影响收敛精度。


\section{结论}
本文提出了一种基于梯度模长的自适应学习率调整策略，通过理论推导证明了算法的收敛性，并通过数值实验验证了其有效性。主要结论如下：
1. 自适应策略能根据梯度变化动态调整学习率，在加速收敛的同时保证稳定性；
2. 实验结果表明，改进算法的平均收敛速度比传统算法提升78.6%；
3. 该策略无需额外计算成本，可直接嵌入现有梯度下降框架，具有较强的实用性。

未来工作可将该策略扩展至随机梯度下降（SGD）和动量梯度下降算法，并在深度学习模型上进行验证。


% 参考文献
\section{参考文献}
\bibliographystyle{plainnat}
\begin{thebibliography}{9} % 手动管理参考文献（无需.bib文件）
    \bibitem{boyd2004convex}
    Boyd, S., \& Vandenberghe, L. (2004). 
    \textit{Convex optimization}. Cambridge university press.
    
    \bibitem{bottou2010large}
    Bottou, L. (2010). 
    Large-scale machine learning with stochastic gradient descent. 
    In \textit{Proceedings of COMPSTAT'2010} (pp. 177-186).
    
    \bibitem{duchi2011adaptive}
    Duchi, J., Hazan, E., \& Singer, Y. (2011). 
    Adaptive subgradient methods for online learning and stochastic optimization. 
    \textit{Journal of Machine Learning Research}, 12(Jul), 2121-2159.
\end{thebibliography}


% 附录：补充公式推导
\appendix
\section{引理2.1的证明}
由Lipschitz条件可知：
\[
f(\theta_{k+1}) \leq f(\theta_k) + \nabla f(\theta_k)^T (\theta_{k+1} - \theta_k) + \frac{L}{2} \|\theta_{k+1} - \theta_k\|^2
\]
代入$\theta_{k+1} = \theta_k - \eta \nabla f(\theta_k)$，得：
\[
f(\theta_{k+1}) \leq f(\theta_k) - \eta \|\nabla f(\theta_k)\|^2 + \frac{L \eta^2}{2} \|\nabla f(\theta_k)\|^2
\]
整理后不等式两边求和，结合$\eta < \frac{2}{L}$即可得证。


\end{document}